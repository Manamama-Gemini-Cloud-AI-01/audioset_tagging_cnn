# AI Analysis Guide for Audioset Tagging CNN Output

**Version:** 1.0
**Purpose:** This guide instructs an AI assistant on how to analyze the output artifacts generated by the `audioset_tagging_cnn_inference_6.py` script. The primary goal is to correlate detected audio events with the visual context of the source media file.

---

## 1. Initial Analysis & Data Sources

Your analysis should always begin by consulting the manifest and the primary data logs.

*   **Step 1.1: Read the Manifest**
    *   **File:** `summary_manifest.json`
    *   **Action:** Read this file first. It is the definitive map of all generated artifacts and their purposes.

*   **Step 1.2: High-Level Querying**
    *   **File:** `summary_events.csv`
    *   **Action:** For broad questions about prominent or continuous sounds, read this file. It provides a quick overview of major sound events and their start/end times.
    *   **Example:** "Find all instances of 'Dog' barking."

*   **Step 1.3: Detailed & Specific Querying**
    *   **File:** `full_event_log.csv`
    *   **Action:** For finding specific, intermittent, or low-probability sounds, use `search_file_content` on this file. This is the complete, granular log of all detected sound probabilities over time.
    *   **Example:** "Search for any 'DTMF' or 'Siren' sounds."

---

## 2. The "How to See" Workflow for Visual Context

To answer questions that require visual understanding, you must obtain a visual representation of the scene at the moment an audio event occurs.

*   **Objective:** Get a textual description of the visual scene corresponding to a specific audio event timestamp.

*   **Step 2.1: Isolate the Event in Time**
    *   Using the methods from Section 1, identify the precise timestamp (in seconds) of the audio event you need to investigate.

*   **Step 2.2: Extract a Still Frame for Analysis**
    *   Use the `ffmpeg` tool to extract a single, representative image frame from the original source video at the identified timestamp.
    *   **Template Command:**
        ```bash
        # Extracts a single JPG frame at the event's timestamp
        ffmpeg -ss <event_timestamp_in_seconds> -i <path_to_source_video> -vframes 1 <path_to_output_frame.jpg>
        ```

*   **Step 2.3: Enable AI Vision (Choose Method A or B)**
    *   Once a frame has been extracted, use one of the following methods to analyze its visual content.

    *   ### Method A: Interactive (User-Assisted Embedding)
        *   **Use Case:** Best for quick, real-time analysis during an interactive session with a user.
        *   **Instruction:** The user must embed the generated image file directly into the prompt. This is the most direct way to activate the AI's core vision capabilities.
        *   **Example Interaction:**
            > **AI:** "I have extracted the frame to `/path/to/frame.jpg`. To analyze it, please embed the file in your next prompt."
            > **User:** "What is happening in this scene? @/path/to/frame.jpg"

    *   ### Method B: Programmatic (Automated Scripting)
        *   **Use Case:** Best for automated workflows or when creating permanent, self-documented artifacts where the description is stored with the image.
        *   **Instruction:** Use the `describe_media.sh` script to generate a description and embed it within the image's EXIF metadata. The AI can then read this metadata tag.
        *   **Step 1 - Generate Description:**
            ```bash
            # This command runs a local VLM on the image and saves the description to its metadata.
            /home/zezen/Downloads/GitHub/Ubuntu_Scripts_1/ai_ml/describe_media.sh /path/to/output_frame.jpg
            ```
        *   **Step 2 - Read Description:**
            ```bash
            # The AI can then read the embedded description using exiftool.
            exiftool -Description -s3 /path/to/output_frame.jpg
            ```

---

## 3. Synthesizing Multi-Modal Insights

The final step is to combine the data from all sources to form a comprehensive conclusion.

*   **Action:** Correlate the audio event data (Section 1) with the textual description of the visual scene (Section 2).
*   **Example Synthesis:** "The audio logs detected a 'DTMF' sound. The visual context, described as 'inside a police vehicle', suggests the sound is not from a personal phone call but is more likely from the officer operating professional equipment like a radio or an in-car computer (MDT)."

---

## 4. File Manifest

The standard output of the `audioset_tagging_cnn` script includes the following files, as described in `summary_manifest.json`:

*   `summary_manifest.json`: Describes all other generated files.
*   `summary_events.csv`: A summary of prominent, continuous sound events.
*   `full_event_log.csv`: A detailed log of sound event probabilities for every time frame.
*   `eventogram.png`: A static visualization of the top 10 sound events over time.
*   `*_eventogram_dynamic.mp4`: A video of the eventogram with the original audio.
*   `*_eventogram_dynamic_overlay.mp4`: The original video with the eventogram visualization overlaid.